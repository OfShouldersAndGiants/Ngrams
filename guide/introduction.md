# Build a Large Language Model

This repository contains the code for developing, pretraining, and finetuning a GPT-like LLM in **Rust**, you can find the book [here](https://amzn.to/4fqvn0D) and the official source code repository [here](https://github.com/rasbt/LLMs-from-scratch).

<br>
<br>

<a href="https://amzn.to/4fqvn0D"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123" width="250px"></a>

<br>

In [*Build a Large Language Model (From Scratch)*](http://mng.bz/orYv), we will learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step.

The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT.

- Link to the official [source code repository](https://github.com/rasbt/LLMs-from-scratch)
- [Link to the book at Manning (the publisher's website)](http://mng.bz/orYv)
- ISBN 9781633437166

<br><br>
<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/mental-model.jpg" width="650px">

<br>
&nbsp;

## Chapter Progress

| Chapter | Link |
|---------|------|
| Chapter 2: Tokenization | [Guide](./guide/chatper-02.md) |
